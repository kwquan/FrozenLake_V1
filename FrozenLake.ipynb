{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake V1\n",
    "### This notebook attempts to solve the FrozenLake-V1 problem[part of OpenAI's gym environments] using Dynamic Programming\n",
    "### Description of problem: \n",
    "### The \"frozen lake\" is either a 4x4 or 8x8 grid environment, where the agent[starting from top left grid] attempts to reach the goal[G] without falling into any holes[H]. All transition rewards=0, except for transition into terminal state[reward=1]. If agent falls into hole, it restarts from initial starting position[S]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create environment \n",
    "#### In this case, we only consider deterministic problem[where action=down will definitely lead to transition to the state below(for example)] in a 8x8 gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating 3 arrays: \\\n",
    "1) state_values: an array of 64 values(initialised to 0) for recording values for each state \\\n",
    "2) new_state_values: an array of 64 values(initialised to 0) for recording new values for each state \\\n",
    "3) actions: an array of 4 values[0,1,2,3](each representing LEFT,DOWN,RIGHT,UP actions]\n",
    "\n",
    "delta is the difference between new & old state values(initialised to 0) \\\n",
    "theta is the min threshold for a value update to be allowed \\\n",
    "discount=gamma \\\n",
    "episodes = no. of complete training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None,map_name=\"8x8\", is_slippery=False, new_step_api=True)\n",
    "\n",
    "observation, info = env.reset(seed=0, return_info=True)\n",
    "state_values = np.zeros(64)\n",
    "new_state_values = np.zeros(64)\n",
    "actions = np.arange(4)\n",
    "delta, theta, discount, episodes = 0, 0.0001, 0.9, 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train agent  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by considering every state[64 in total] \\\n",
    "For each state, we create a list(q_values) to store q_value for every state-action pair \\\n",
    "For each action, we pass state & action to env.env.P to obtain a series of transitions \\\n",
    "For each transition, we get prob, next_state, reward, terminal \\\n",
    "We will use prob, next_state & reward to increment that particular state-action pair's value \\\n",
    "\n",
    "Once completed, we calculate delta(absolute difference between old state value & max(q_values) \\\n",
    "If delta exceeds threshold(theta), we then use it as our new state value \\\n",
    "After each complete sweep, we replace state_values with new_state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1083.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweep for 1000 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(episodes)):\n",
    "    for state in range(len(state_values)):\n",
    "        q_values = np.zeros(4)\n",
    "        for action in range(len(actions)):\n",
    "            transitions = env.env.P[state][action]\n",
    "            for transition in transitions:\n",
    "                prob, next_state, reward, terminal = transition\n",
    "                q_values[action] += prob*(reward + discount*state_values[next_state])\n",
    "        delta = np.abs(max(q_values) - state_values[state])\n",
    "        if delta > theta:\n",
    "            new_state_values[state] = max(q_values)\n",
    "    state_values = new_state_values        \n",
    "print(f\"Sweep for {episodes} episodes done\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy optimization is done after completion of agent training \\\n",
    "Similar steps are followed, with the exception of selecting the index of the action that gives max(q_values) \\\n",
    "If there is a tie, we use np.random.choice to select at random \\\n",
    "Since index is also the action, selected index will be the optimal action for that state \\\n",
    "Finally, update policy for that state using optimal action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = np.zeros(64)\n",
    "for state in range(len(state_values)):\n",
    "    q_values = np.zeros(4)\n",
    "    for action in range(len(actions)):\n",
    "        transitions = env.env.P[state][action]\n",
    "        for transition in transitions:\n",
    "            prob, next_state, reward, terminal = transition\n",
    "            q_values[action] += prob*(reward + discount*state_values[next_state])\n",
    "    optimal_action = np.random.choice([idx for idx in range(len(q_values)) if q_values[idx] == max(q_values)])\n",
    "    optimal_policy[state] = optimal_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of state_values, reshaped as 8x8 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25418658, 0.28242954, 0.3138106 , 0.34867844, 0.38742049,\n",
       "        0.43046721, 0.4782969 , 0.531441  ],\n",
       "       [0.28242954, 0.3138106 , 0.34867844, 0.38742049, 0.43046721,\n",
       "        0.4782969 , 0.531441  , 0.59049   ],\n",
       "       [0.3138106 , 0.34867844, 0.38742049, 0.        , 0.4782969 ,\n",
       "        0.531441  , 0.59049   , 0.6561    ],\n",
       "       [0.34867844, 0.38742049, 0.43046721, 0.4782969 , 0.531441  ,\n",
       "        0.        , 0.6561    , 0.729     ],\n",
       "       [0.3138106 , 0.34867844, 0.38742049, 0.        , 0.59049   ,\n",
       "        0.6561    , 0.729     , 0.81      ],\n",
       "       [0.28242954, 0.        , 0.        , 0.59049   , 0.6561    ,\n",
       "        0.729     , 0.        , 0.9       ],\n",
       "       [0.3138106 , 0.        , 0.4782969 , 0.531441  , 0.        ,\n",
       "        0.81      , 0.        , 1.        ],\n",
       "       [0.34867844, 0.38742049, 0.43046721, 0.        , 0.81      ,\n",
       "        0.9       , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_values.reshape(8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rendering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we observe our trained agent in action \\\n",
    "Notice how our agent is able to find an optimal path to the goal, while avoiding all holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None,map_name=\"8x8\", is_slippery=False, render_mode='human')\n",
    "observation, info = env.reset(seed=0, return_info=True)\n",
    "env.render()\n",
    "for episode in range(100):\n",
    "    state=observation\n",
    "    observation, reward, done, info = env.step(action = int(optimal_policy[state]))\n",
    "    env.render()\n",
    "    \n",
    "    if done:  \n",
    "        break\n",
    "        \n",
    "env.close()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated, the usage of dynamic programming is able to find an optimal solution for this simple 8x8 gridworld problem \\\n",
    "However, do note that DP is seldom used due to it's computational complexity although it is important to understand it's concepts since they will be further developed upon in other methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

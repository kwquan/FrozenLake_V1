{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake V1\n",
    "### This notebook attempts to solve the FrozenLake-V1 problem[part of OpenAI's gym environments] using Dynamic Programming & Every-Visit Monte Carlo\n",
    "### Description of problem: \n",
    "### The \"frozen lake\" is either a 4x4 or 8x8 grid environment, where the agent[starting from top left grid] attempts to reach the goal[G] without falling into any holes[H]. All transition rewards=0, except for transition into terminal state[reward=1]. If agent falls into hole, it restarts from initial starting position[S]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dynamic Programming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create environment \n",
    "#### In this case, we only consider deterministic problem[where action=down will definitely lead to transition to the state below(for example)] in a 8x8 gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating 3 arrays: \\\n",
    "1) state_values: an array of 64 values(initialised to 0) for recording values for each state \\\n",
    "2) new_state_values: an array of 64 values(initialised to 0) for recording new values for each state \\\n",
    "3) actions: an array of 4 values[0,1,2,3](each representing LEFT,DOWN,RIGHT,UP actions]\n",
    "\n",
    "delta is the difference between new & old state values(initialised to 0) \\\n",
    "theta is the min threshold for a value update to be allowed \\\n",
    "discount=gamma \\\n",
    "episodes = no. of complete training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None,map_name=\"8x8\", is_slippery=False, new_step_api=True)\n",
    "\n",
    "observation, info = env.reset(seed=0, return_info=True)\n",
    "state_values = np.zeros(64)\n",
    "new_state_values = np.zeros(64)\n",
    "actions = np.arange(4)\n",
    "delta, theta, discount, episodes = 0, 0.0001, 0.9, 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train agent  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by considering every state[64 in total] \\\n",
    "For each state, we create a list(q_values) to store q_value for every state-action pair \\\n",
    "For each action, we pass state & action to env.env.P to obtain a series of transitions \\\n",
    "For each transition, we get prob, next_state, reward, terminal \\\n",
    "We will use prob, next_state & reward to increment that particular state-action pair's value \\\n",
    "\n",
    "Once completed, we calculate delta(absolute difference between old state value & max(q_values) \\\n",
    "If delta exceeds threshold(theta), we then use it as our new state value \\\n",
    "After each complete sweep, we replace state_values with new_state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm(range(episodes)):\n",
    "    for state in range(len(state_values)):\n",
    "        q_values = np.zeros(4)\n",
    "        for action in range(len(actions)):\n",
    "            transitions = env.env.P[state][action]\n",
    "            for transition in transitions:\n",
    "                prob, next_state, reward, terminal = transition\n",
    "                q_values[action] += prob*(reward + discount*state_values[next_state])\n",
    "        delta = np.abs(max(q_values) - state_values[state])\n",
    "        if delta > theta:\n",
    "            new_state_values[state] = max(q_values)\n",
    "    state_values = new_state_values        \n",
    "print(f\"Sweep for {episodes} episodes done\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy optimization is done after completion of agent training \\\n",
    "Similar steps are followed, with the exception of selecting the index of the action that gives max(q_values) \\\n",
    "If there is a tie, we use np.random.choice to select at random \\\n",
    "Since index is also the action, selected index will be the optimal action for that state \\\n",
    "Finally, update policy for that state using optimal action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = np.zeros(64)\n",
    "for state in range(len(state_values)):\n",
    "    q_values = np.zeros(4)\n",
    "    for action in range(len(actions)):\n",
    "        transitions = env.env.P[state][action]\n",
    "        for transition in transitions:\n",
    "            prob, next_state, reward, terminal = transition\n",
    "            q_values[action] += prob*(reward + discount*state_values[next_state])\n",
    "    optimal_action = np.random.choice([idx for idx in range(len(q_values)) if q_values[idx] == max(q_values)])\n",
    "    optimal_policy[state] = optimal_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of state_values, reshaped as 8x8 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_values.reshape(8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rendering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we observe our trained agent in action \\\n",
    "Notice how our agent is able to find an optimal path to the goal, while avoiding all holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None,map_name=\"8x8\", is_slippery=False, render_mode='human')\n",
    "observation, info = env.reset(seed=0, return_info=True)\n",
    "env.render()\n",
    "for episode in range(100):\n",
    "    state=observation\n",
    "    observation, reward, done, info = env.step(action = int(optimal_policy[state]))\n",
    "    env.render()\n",
    "    \n",
    "    if done:  \n",
    "        break\n",
    "        \n",
    "env.close()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated, the usage of dynamic programming is able to find an optimal solution for this simple 8x8 gridworld problem \\\n",
    "However, do note that DP is seldom used due to it's computational complexity although it is important to understand it's concepts since they will be further developed upon in other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Every-Visit Monte Carlo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we attempt every-visit monte carlo on the same environment \\\n",
    "Logic is as followed:\n",
    "For each episode, \\\n",
    "1) Calculate action_probability[action_prob]. If it's < epsilon, take exploratory action. Otherwise, act greedily \\\n",
    "2) Pass action to env.step() to obtain next_state & reward \\\n",
    "3) Append [state,action,reward] to episode list \\\n",
    "4) Update state as next_state\n",
    "\n",
    "Upon termination: \\\n",
    "Initialise return[G]=0 \\\n",
    "To facilitate return calculation for every state-action pair, we work backwards \\\n",
    "Starting from last state-action pair, set it's reward[G[last]]=reward[last] \\\n",
    "2nd-last state-action pair will have G[2nd-last] = reward[2nd-last] + G[last] \\\n",
    "3rd-last state-action pair will have G[3rd-last] = reward[3rd-last] + g[2nd-last] \\\n",
    "And so on...\n",
    "\n",
    "For each state-action pair visited:\n",
    "append it's G to returns[(state,action)] \\\n",
    "returns is a dictionary containing all Gs for every state-action pair visited \\\n",
    "Idea is to take average of each returns[(state,action)] as new action-values \n",
    "\n",
    "Incremental update: \\\n",
    "Instead of using sum(returns[(state,action)])/len(returns[(state,action)]) for means calculation, \\\n",
    "we can use returns[(s, a)] + (returns[(s, a)][-1] - action_values[s, a])/len(returns[(s, a)]) to get same result faster\n",
    "\n",
    "New episode: \\\n",
    "After each episode, reset terminate=False, reset env{env.reset()) & reset episode=[] \n",
    "\n",
    "Policy update: \\\n",
    "policy list is an array of size (,64), each representing the optimal action for each state \\\n",
    "Loop through each state & take argmax of each state's action_value to get optimal action \\\n",
    "Ties are broken randomly \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None,map_name=\"8x8\", is_slippery=False, new_step_api=True)\n",
    "\n",
    "state, info = env.reset(seed=0, return_info=True)\n",
    "action_values = np.zeros((64,4))\n",
    "policy = np.zeros(64)\n",
    "episodes = 40000\n",
    "epsilon = 0.01\n",
    "total_rewards = []\n",
    "average_rewards = []\n",
    "terminate = False\n",
    "returns = {}\n",
    "episode = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [31:31<00:00, 21.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent training for 40000 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(episodes)):\n",
    "    while not terminate:\n",
    "        action_prob = np.random.uniform()\n",
    "        if action_prob < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.random.choice([idx for idx in range(4) if action_values[state,idx] == max(action_values[state])])\n",
    "        next_state, reward, terminate, truncated, info = env.step(action)\n",
    "        episode.append([state,action,reward])\n",
    "        total_rewards.append(reward)\n",
    "        state = next_state  \n",
    "    G = 0\n",
    "    for i in reversed(range(len(episode))):\n",
    "        s,a,r = episode[i]\n",
    "        G += r\n",
    "        if returns.get((s,a)):\n",
    "            returns[(s,a)].append(G)\n",
    "        else:\n",
    "            returns[(s,a)] = [G]\n",
    "                \n",
    "\n",
    "    #action_values[s, a] = sum(returns[(s,a)])/len(returns[(s,a)])\n",
    "        action_values[s, a] += (returns[(s, a)][-1] - action_values[s, a])/len(returns[(s, a)])\n",
    "    \n",
    "    for j in range(len(policy)-1):\n",
    "        policy[j] = np.random.choice([idx for idx in range(4) if action_values[j,idx] == max(action_values[j])])\n",
    "        \n",
    "    state = env.reset() \n",
    "    episode = []  \n",
    "    terminate = False\n",
    "    \n",
    "    if i%100==0:\n",
    "#         print(f\"average reward for batch {i%100}: {sum(total_rewards)/len(total_rewards)}\")\n",
    "        average_rewards.append(sum(total_rewards)/len(total_rewards))\n",
    "        \n",
    "print(f\"agent training for {episodes} episodes done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot average rewards over time[calculated for every ith batch] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate average rewards everytime we hit i%100 == 0 & plot it on a graph \\\n",
    "Observe that average rewards are increasing over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(average_rewards)\n",
    "plt.xlabel(\"Episode Batch\")\n",
    "plt.ylabel(\"Average \\n Rewards\",rotation=0)\n",
    "plt.ylim(0,round(max(average_rewards),4))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 2., 2., 2., 2., 1., 2., 1.],\n",
       "       [3., 1., 3., 3., 2., 2., 1., 3.],\n",
       "       [3., 3., 0., 1., 1., 2., 2., 1.],\n",
       "       [1., 0., 0., 0., 0., 2., 2., 1.],\n",
       "       [3., 3., 0., 3., 3., 0., 3., 1.],\n",
       "       [3., 2., 0., 2., 3., 0., 2., 1.],\n",
       "       [3., 0., 3., 1., 3., 2., 3., 1.],\n",
       "       [2., 3., 1., 1., 3., 0., 3., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.reshape(8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rendering[If needed] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None,map_name=\"8x8\", is_slippery=False, render_mode='human')\n",
    "observation, info = env.reset(seed=0, return_info=True)\n",
    "env.render()\n",
    "for episode in range(100):\n",
    "    state=observation\n",
    "    observation, reward, done, info = env.step(action = int(policy[state]))\n",
    "    env.render()\n",
    "    \n",
    "    if done:  \n",
    "        break\n",
    "        \n",
    "env.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "As observed from the final policy, every-visit MC does not give a perfect solution with current params \\\n",
    "Draw out the path given by optimal policy on a 8x8 grid to have a clearer picture \\\n",
    "Although most most states have policies pointing in the \"right\" directions, we observe a few states with incorrect policies[that does not point in the optimal direction] \\\n",
    "With more episodes however, we should be able to correct this issue and achieve a good solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
